{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "NLP - or Natural Language Processing ou processamento de linguagem natural é de uma forma simples é um conjunto amplo de técnica desenvolvidas para ajudar as máquinas a aprender textos. A NLP da vida de chatbots a motores de busca e é usado para diversas tarefas como análise de sentimentos e tradução. \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "#IMPORTANDO LIBS\n",
    "import numpy as np # linear algebra\n",
    "import pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n",
    "from sklearn import feature_extraction, linear_model, model_selection, preprocessing\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# IMPORTANDO DADOS\n",
    "train_df = pd.read_csv(\"input/nlp-getting-start/train.csv\")\n",
    "test_df = pd.read_csv(\"input/nlp-getting-start/test.csv\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>keyword</th>\n",
       "      <th>location</th>\n",
       "      <th>text</th>\n",
       "      <th>target</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Our Deeds are the Reason of this #earthquake M...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>4</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Forest fire near La Ronge Sask. Canada</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>5</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>All residents asked to 'shelter in place' are ...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>6</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>13,000 people receive #wildfires evacuation or...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>7</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Just got sent this photo from Ruby #Alaska as ...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>8</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>#RockyFire Update =&gt; California Hwy. 20 closed...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>10</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>#flood #disaster Heavy rain causes flash flood...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>13</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>I'm on top of the hill and I can see a fire in...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>14</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>There's an emergency evacuation happening now ...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>15</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>I'm afraid that the tornado is coming to our a...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   id keyword location                                               text  \\\n",
       "0   1     NaN      NaN  Our Deeds are the Reason of this #earthquake M...   \n",
       "1   4     NaN      NaN             Forest fire near La Ronge Sask. Canada   \n",
       "2   5     NaN      NaN  All residents asked to 'shelter in place' are ...   \n",
       "3   6     NaN      NaN  13,000 people receive #wildfires evacuation or...   \n",
       "4   7     NaN      NaN  Just got sent this photo from Ruby #Alaska as ...   \n",
       "5   8     NaN      NaN  #RockyFire Update => California Hwy. 20 closed...   \n",
       "6  10     NaN      NaN  #flood #disaster Heavy rain causes flash flood...   \n",
       "7  13     NaN      NaN  I'm on top of the hill and I can see a fire in...   \n",
       "8  14     NaN      NaN  There's an emergency evacuation happening now ...   \n",
       "9  15     NaN      NaN  I'm afraid that the tornado is coming to our a...   \n",
       "\n",
       "   target  \n",
       "0       1  \n",
       "1       1  \n",
       "2       1  \n",
       "3       1  \n",
       "4       1  \n",
       "5       1  \n",
       "6       1  \n",
       "7       1  \n",
       "8       1  \n",
       "9       1  "
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_df.head(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Exemplo de tweets normais"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'I love fruits'"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_df[train_df[\"target\"] == 0][\"text\"].values[1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([\"What's up man?\", 'I love fruits', 'Summer is lovely',\n",
       "       'My car is so fast', 'What a goooooooaaaaaal!!!!!!'], dtype=object)"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_df[train_df[\"target\"] == 0][\"text\"].values[0:5]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Exemplo de um tweet de desastre:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Forest fire near La Ronge Sask. Canada'"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_df[train_df[\"target\"] == 1][\"text\"].values[1]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Construindo vetores\n",
    "A teoria por trás deste modelo é bem simples: as palavras contendo cada tweet são bons indicadores se eles são reais desastres ou não. (Não está inteiramente correto mas é um bom lugar pra começar)\n",
    "\n",
    "Para isso podemos usar a biblioteca CountVectorizer do scikit-learn para contar as palavras em cada tweet e transformá-las em dados nosso modelo de machine learning possa ser processado. \n",
    "\n",
    "O vetor é nesse contexto, é um conjunto de numeros que o modelo de machine learning pode trabalhar. \n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "count_vectorizer = feature_extraction.text.CountVectorizer()\n",
    "\n",
    "## let's get counts for the first 5 tweets in the data\n",
    "example_train_vectors = count_vectorizer.fit_transform(train_df[\"text\"][0:5])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  (0, 34)\t1\n",
      "  (0, 12)\t1\n",
      "  (0, 5)\t1\n",
      "  (0, 49)\t1\n",
      "  (0, 39)\t1\n",
      "  (0, 29)\t1\n",
      "  (0, 50)\t1\n",
      "  (0, 13)\t1\n",
      "  (0, 25)\t1\n",
      "  (0, 4)\t1\n",
      "  (0, 18)\t1\n",
      "  (0, 52)\t1\n",
      "  (0, 3)\t1\n",
      "  (1, 17)\t1\n",
      "  (1, 16)\t1\n",
      "  (1, 26)\t1\n",
      "  (1, 24)\t1\n",
      "  (1, 42)\t1\n",
      "  (1, 44)\t1\n",
      "  (1, 11)\t1\n",
      "  (2, 5)\t2\n",
      "  (2, 3)\t1\n",
      "  (2, 41)\t1\n",
      "  (2, 7)\t1\n",
      "  (2, 51)\t1\n",
      "  :\t:\n",
      "  (2, 32)\t1\n",
      "  (2, 15)\t1\n",
      "  (3, 21)\t1\n",
      "  (3, 14)\t1\n",
      "  (3, 32)\t1\n",
      "  (3, 1)\t1\n",
      "  (3, 0)\t1\n",
      "  (3, 35)\t1\n",
      "  (3, 40)\t1\n",
      "  (3, 53)\t1\n",
      "  (3, 10)\t1\n",
      "  (4, 50)\t1\n",
      "  (4, 53)\t1\n",
      "  (4, 23)\t1\n",
      "  (4, 20)\t1\n",
      "  (4, 46)\t1\n",
      "  (4, 36)\t1\n",
      "  (4, 19)\t2\n",
      "  (4, 43)\t1\n",
      "  (4, 2)\t1\n",
      "  (4, 6)\t1\n",
      "  (4, 48)\t1\n",
      "  (4, 38)\t1\n",
      "  (4, 22)\t1\n",
      "  (4, 45)\t1\n"
     ]
    }
   ],
   "source": [
    "print(example_train_vectors)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(1, 54)\n",
      "[[0 0 0 1 1 1 0 0 0 0 0 0 1 1 0 0 0 0 1 0 0 0 0 0 0 1 0 0 0 1 0 0 0 0 1 0\n",
      "  0 0 0 1 0 0 0 0 0 0 0 0 0 1 1 0 1 0]]\n"
     ]
    }
   ],
   "source": [
    "## usar o .todense() pois os vetores são  enxutos apenas elementos diferente de zero são salvos.\n",
    "print(example_train_vectors[0].todense().shape)\n",
    "print(example_train_vectors[0].todense())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A informação acima mostra que, há 54 palavras únicas (ou tokens) nos primeiros 5 tweets. \n",
    "\n",
    "O primeiro tweet contem apenas alguns desses tokens únicos, todos os contadores que não são zero acima são tokes que existem no primeiro tweet. Agora vamos criar um vetor para todos os tweets.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_vectors = count_vectorizer.fit_transform(train_df[\"text\"])\n",
    "\n",
    "## note that we're NOT using .fit_transform() here. Using just .transform() makes sure\n",
    "# that the tokens in the train vectors are the only ones mapped to the test vectors - \n",
    "# i.e. that the train and test vectors use the same set of tokens.\n",
    "test_vectors = count_vectorizer.transform(test_df[\"text\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Nosso modelo\n",
    "\n",
    "Como mencionado antes, podemos pensar nas palavras contidas em cada tweet são bons indicadores se eles são sobre desastres reais ou não. A presença de uma palavra em particular (ou o conjunto de palavras) em um tweet deve estar relacionado diretamente se o tweet é real ou não. \n",
    "\n",
    "O que estamos assumindo aqui é uma correlação linear, então vamos construir um modelo linear e ver.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Our vectors are really big, so we want to push our model's weights\n",
    "## toward 0 without completely discounting different words - ridge regression \n",
    "## is a good way to do this.\n",
    "clf = linear_model.RidgeClassifier()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Vamos testar nosso modelo e ver quão bem ele está em relação aos dados de treino. Para isso, vamos usar a cross-validation, onde treinamos parte dos dados conhecidos e validamos com o restante. Se fizermos isso algumas vezes (com porções diferentes) podemos ter uma boa ideia de como um modelo particular ou método performa. \n",
    "\n",
    "A métrica para esta competição é F1, então vamos usá-la aqui.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0.59421842, 0.56498283, 0.64113893])"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "scores = model_selection.cross_val_score(clf, train_vectors, train_df[\"target\"], cv=3, scoring=\"f1\")\n",
    "scores\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Os scores acimas não são terríveis! Parecessem que assumimos um score próximo a 0.65 no leaderboard. Existem várias formas de potencializar e melhorar isso (TFIDF, LSA, LSTM / RNNs, ..) - Faça um teste!\n",
    "\n",
    "Neste meio tempo, faremos previsões no nosso conjunto de treino e vamos construir dados de envio para a competição. \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "RidgeClassifier(alpha=1.0, class_weight=None, copy_X=True, fit_intercept=True,\n",
       "                max_iter=None, normalize=False, random_state=None,\n",
       "                solver='auto', tol=0.001)"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "clf.fit(train_vectors, train_df[\"target\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "sample_submission = pd.read_csv(\"input/nlp-getting-start/sample_submission.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "sample_submission[\"target\"] = clf.predict(test_vectors)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>target</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>3</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>9</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>11</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   id  target\n",
       "0   0       0\n",
       "1   2       1\n",
       "2   3       1\n",
       "3   9       0\n",
       "4  11       1"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sample_submission.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "sample_submission.to_csv(\"submission.csv\", index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## TFIDF\n",
    "Tentar melhorar os dados usando TF-IDF"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "\n",
    "vectorized = TfidfVectorizer()\n",
    "train_vector_tfid = vectorized.fit_transform(train_df[\"text\"])\n",
    "\n",
    "scores = model_selection.cross_val_score(clf, train_vector_tfid, train_df[\"target\"], cv=3, scoring=\"f1\")\n",
    "scores\n",
    "\n",
    "sample_submission = pd.read_csv(\"input/nlp-getting-start/sample_submission.csv\")\n",
    "sample_submission[\"target\"] = clf.predict(test_vectors)\n",
    "sample_submission.to_csv(\"output/nlp-getting-start/submission.csv\", index=False)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Aplicando o tfid, que é feito para trabalhar NPL já ficou melhor. Uma boa explicação desse método encontrei no FCC: https://www.freecodecamp.org/news/how-to-process-textual-data-using-tf-idf-in-python-cd2bbc0a94a3/"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## LSA\n",
    "\n",
    "Latent Semantic Analysis, sumarização automática de textos. Identifica, coleta ou gera sentenças relevantes a partir de documentos textuais, produzindo ao final um sumário.  Sumários caracterizam-se por serem construídos de modo a não perderem consideravelmente o significado original, apesar de conterem menos informações e poderem apresentar diferentes estruturas, em relação a suas fontes. Métodos que coletam palavras e sentenças diretamente do texto e as agrupam no resumo sem preocupação com reescrita são chamados de extrativos. Já métodos que usam uma representação linguística interna para gerar resumos mais humanos, parafraseando a intenção do texto original, são chamados de abstratos. Além de considerar as palavras mais frequentes, tratava três componentes adicionais: palavras pragmáticas (cue words); palavras de título e título; e indicadores estruturais (localização da sentença no texto). O sistema de Edmundson era parametrizado de acordo com a influência de todas estas heurísticas, atribuindo uma pontuação (positiva ou negativa) à sentença. Depois que todas as heurísticas eram aplicadas, as frases de maior pontuação eram incluídas no resumo.\n",
    "\n",
    "Outra técnica usada em sumarização automática de textos chama-se LSA – Latent Semantic Analysis. O LSA analisa documentos para encontrar os significados ou conceitos subjacentes. É um método que se originou em outra área de processamento de linguagem natural, chamada recuperação de informações, na qual palavras são informadas como argumentos para a recuperação de textos.\n",
    "\n",
    "O LSA parte do pressuposto de que uma palavra descreve um conceito e que os autores dos textos escolhem palavras para representar estes conceitos. Selecionar as sentenças de um texto significa, portanto, selecionar os conceitos mais importantes, através das palavras que os representam. O algoritmo do LSA identifica quais palavras são usadas em conjunto e quais palavras comuns são vistas em frases diferentes. Uma grande quantidade de palavras comuns entre frases diferentes indica que as frases são semanticamente relacionadas.\n",
    "\n",
    "Existem técnicas que utilizam grafos para representação das sentenças. É o caso do TextRank, um algoritmo não supervisionado, baseado em grafos, usado para sumarização automática de textos, que também pode ser usado ​​para obter as palavras-chave de um documento. Foi introduzido por Rada Mihalcea e Paul Tarau [6] a partir de uma variação do algoritmo PageRank.\n",
    "\n",
    "Este modelo de classificação baseia-se na ideia de votação ou recomendação. Pode-se imaginar que quando um vértice aponta para outro, é basicamente um voto que esta sendo dado para este outro vértice. Assim, quanto maior o número de votos que o vértice recebe, maior é a importância que ele tem. Além disso, o algoritmo também considera a importância do vértice do qual se está recebendo o voto. Se o vértice que vota também recebe muitos votos, ele tem importância maior do que outro vértice que recebe poucos votos. Os vértices podem representar palavras, n-Gramas ou sentenças completas.\n",
    "\n",
    "## LSA em Python\n",
    "\n",
    "A bilbioteca em python se chama Sumy e abaixo segue o código de como utilizá-la no exemplo anterior."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sumy\n",
    "from sumy.parsers.plaintext import PlaintextParser\n",
    "from sumy.nlp.tokenizers import Tokenizer\n",
    "from sumy.summarizers.lex_rank import LexRankSummarizer\n",
    "from sumy.summarizers.lsa import LsaSummarizer\n",
    "\n",
    "for i in range(len(train_df[\"text\"])):\n",
    "    parser = PlaintextParser.from_string(train_df[\"text\"][i],Tokenizer(\"english\"))\n",
    "    summarizer_lsa = LsaSummarizer()\n",
    "    summary_2 =summarizer_lsa(parser.document,2)\n",
    "\n",
    "    for sentence in summary_2:\n",
    "        print(sentence)\n",
    "    \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Problemas ao instalar o sumy, codigo acima não funcionou."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Criar um pré-processamento de texto para verificar se consigo melhorar um pouco mais os scores:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import nltk\n",
    "nltk.download()\n",
    "\n",
    "from nltk.tokenize import RegexpTokenizer\n",
    "from nltk.corpus import stopwords\n",
    "import nltk\n",
    "from nltk.corpus import wordnet\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "nltk.download('wordnet')\n",
    "\n",
    "wordnet_lemmatizer = WordNetLemmatizer()\n",
    "tokenizer = RegexpTokenizer(r'[a-z]+')\n",
    "stop_words = set(stopwords.words('english'))\n",
    "\n",
    "def preprocess(document):\n",
    "    document = document.lower() # Convert to lowercase\n",
    "    words = tokenizer.tokenize(document) # Tokenize\n",
    "    words = [w for w in words if not w in stop_words] # Removing stopwords\n",
    "    # Lemmatizing\n",
    "    for pos in [wordnet.NOUN, wordnet.VERB, wordnet.ADJ, wordnet.ADV]:\n",
    "        words = [wordnet_lemmatizer.lemmatize(x, pos) for x in words]\n",
    "    return \" \".join(words)\n",
    "\n",
    "print(train_df['text'].apply(preprocess))\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
